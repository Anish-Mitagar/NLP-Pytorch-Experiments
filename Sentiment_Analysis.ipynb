{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [02:39, 5.39MB/s]                               \n",
      "100%|█████████▉| 399999/400000 [00:06<00:00, 64892.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 400000 words in the vocabulary\n"
     ]
    }
   ],
   "source": [
    "glove = torchtext.vocab.GloVe(name='6B', dim=100)\n",
    "\n",
    "print(f'There are {len(glove.itos)} words in the vocabulary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400000, 100])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " ',',\n",
       " '.',\n",
       " 'of',\n",
       " 'to',\n",
       " 'and',\n",
       " 'in',\n",
       " 'a',\n",
       " '\"',\n",
       " \"'s\",\n",
       " 'for',\n",
       " '-',\n",
       " 'that',\n",
       " 'on',\n",
       " 'is']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.itos[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.stoi['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36623"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.stoi['dazzle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(embeddings, word):\n",
    "\n",
    "    assert word in embeddings.stoi, f'*{word}* is not in the vocab!'\n",
    "\n",
    "    return embeddings.vectors[embeddings.stoi[word]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.8503,  0.3336, -0.6589, -0.4987,  0.3659, -0.1925,  0.2566, -0.0534,\n",
       "         0.3147,  0.2443,  0.2934, -0.4492,  0.1517,  0.3931, -0.3179,  0.0605,\n",
       "         0.8177, -0.3885,  0.7676, -1.1041, -0.1544,  0.3165, -0.3724, -0.1148,\n",
       "         0.5163, -0.3929,  0.1630, -0.2532, -0.5098,  0.1520,  0.2781,  0.5252,\n",
       "        -0.3882, -0.3472, -0.6182,  0.1702,  0.1225, -0.2419, -0.3888, -0.5318,\n",
       "        -0.4699, -0.7050, -0.6213, -0.3869, -0.8564, -0.4100, -0.4749, -0.2108,\n",
       "        -0.8134, -0.5240,  0.4989,  0.3791,  0.5543,  1.1230, -0.4212, -1.5674,\n",
       "        -0.5689,  0.4082,  1.7949,  0.1686, -0.0029,  0.2879, -0.9009, -0.0942,\n",
       "         0.7999, -0.3910,  0.7629,  0.7131,  0.1319, -0.4076, -0.1869,  0.8956,\n",
       "         0.4687, -0.0029,  0.0253,  1.0084,  0.1714,  0.5974, -1.1003,  0.4931,\n",
       "         0.4178,  0.1728, -0.4947,  0.0878, -0.9669, -1.0920,  0.3390, -0.5129,\n",
       "         0.2464,  0.2714,  0.2421, -0.2171,  0.5504,  0.0082, -0.4557,  0.1353,\n",
       "        -0.0431, -0.4141,  0.7005,  0.1877])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_vector(glove, 'paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest(embeddings, vector, n = 6):\n",
    "\n",
    "    distances = []\n",
    "\n",
    "    for neighbor in embeddings.itos:\n",
    "        distances.append((neighbor, torch.dist(vector, get_vector(embeddings, neighbor))))\n",
    "\n",
    "    return sorted(distances, key = lambda x: x[1])[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('paper', tensor(0.)),\n",
       " ('papers', tensor(3.8442)),\n",
       " ('printed', tensor(4.1970)),\n",
       " ('print', tensor(4.2666)),\n",
       " ('sheet', tensor(4.3835)),\n",
       " ('printing', tensor(4.4179))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest(glove, get_vector(glove, 'paper'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('shenanigans', tensor(0.)),\n",
       " ('chicanery', tensor(2.3785)),\n",
       " ('hijinks', tensor(2.6764)),\n",
       " ('escapades', tensor(2.7821)),\n",
       " ('machinations', tensor(2.8699)),\n",
       " ('gamesmanship', tensor(2.9044))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest(glove, get_vector(glove, 'shenanigans'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tuples(tuples):\n",
    "\n",
    "    for t in tuples:\n",
    "        print('(%.4f) %s' % (t[1], t[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0000) stupendous\n",
      "(2.5795) marvellous\n",
      "(2.7539) frightful\n",
      "(2.8506) stupefying\n",
      "(2.8561) awe-inspiring\n",
      "(2.9179) mind-blowing\n"
     ]
    }
   ],
   "source": [
    "print_tuples(closest(glove, get_vector(glove, 'stupendous')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(embeddings, w1, w2, w3, n = 6):\n",
    "\n",
    "    print('\\n[%s : %s :: %s : ?]' % (w1, w2, w3))\n",
    "\n",
    "    closest_words = closest(embeddings, get_vector(embeddings, w2) - get_vector(embeddings, w1) + get_vector(embeddings, w3), n + 3)\n",
    "\n",
    "    closest_words = [x for x in closest_words if x[0] not in [w1, w2, w3]][:n]\n",
    "\n",
    "    return closest_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[moon : night :: sun : ?]\n",
      "(5.7069) morning\n",
      "(5.7276) afternoon\n",
      "(5.8023) evening\n",
      "(6.1410) hours\n",
      "(6.2797) saturday\n",
      "(6.3056) sunday\n"
     ]
    }
   ],
   "source": [
    "print_tuples(analogy(glove, 'moon', 'night', 'sun'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[fly : bird :: swim : ?]\n",
      "(5.9754) swimming\n",
      "(6.2409) shark\n",
      "(6.4822) dolphin\n",
      "(6.5421) whale\n",
      "(6.6276) cat\n",
      "(6.6457) gorilla\n"
     ]
    }
   ],
   "source": [
    "print_tuples(analogy(glove, 'fly', 'bird', 'swim'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[earth : moon :: sun : ?]\n",
      "(6.2294) lee\n",
      "(6.4125) kang\n",
      "(6.4644) tan\n",
      "(6.4757) yang\n",
      "(6.4853) lin\n",
      "(6.5220) chong\n"
     ]
    }
   ],
   "source": [
    "print_tuples(analogy(glove, 'earth', 'moon', 'sun'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.4.1-cp39-cp39-macosx_11_0_arm64.whl (6.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from spacy) (4.64.0)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.6-cp39-cp39-macosx_11_0_arm64.whl (31 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.4-cp39-cp39-macosx_11_0_arm64.whl (458 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m458.6/458.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.9\n",
      "  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: setuptools in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from spacy) (61.2.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from spacy) (1.22.3)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.6-cp39-cp39-macosx_11_0_arm64.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4\n",
      "  Downloading pydantic-1.9.1-cp39-cp39-macosx_11_0_arm64.whl (2.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from spacy) (21.3)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.2-py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting thinc<8.2.0,>=8.1.0\n",
      "  Downloading thinc-8.1.0-cp39-cp39-macosx_11_0_arm64.whl (709 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m709.9/709.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting jinja2\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.3-py3-none-any.whl (9.3 kB)\n",
      "Collecting wasabi<1.1.0,>=0.9.1\n",
      "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.7-cp39-cp39-macosx_11_0_arm64.whl (19 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from spacy) (2.28.1)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.2-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Collecting smart-open<6.0.0,>=5.2.1\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.6/58.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy) (4.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.8-cp39-cp39-macosx_11_0_arm64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click<9.0.0,>=7.1.1 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.1.3)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-2.1.1-cp39-cp39-macosx_10_9_universal2.whl (17 kB)\n",
      "Installing collected packages: wasabi, murmurhash, cymem, typer, spacy-loggers, spacy-legacy, smart-open, pydantic, preshed, MarkupSafe, langcodes, catalogue, blis, srsly, pathy, jinja2, thinc, spacy\n",
      "Successfully installed MarkupSafe-2.1.1 blis-0.7.8 catalogue-2.0.8 cymem-2.0.6 jinja2-3.1.2 langcodes-3.3.0 murmurhash-1.0.7 pathy-0.6.2 preshed-3.0.6 pydantic-1.9.1 smart-open-5.2.1 spacy-3.4.1 spacy-legacy-3.0.9 spacy-loggers-1.0.3 srsly-2.4.4 thinc-8.1.0 typer-0.4.2 wasabi-0.10.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
      "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "Collecting en-core-web-sm==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from en-core-web-sm==3.4.0) (3.4.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.64.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.28.1)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.1.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n",
      "Requirement already satisfied: setuptools in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (61.2.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.22.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.7)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.8)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.4.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.1'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext import datasets\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gt/sbz9rptd4qb0s6w3gb_sy48c0000gn/T/ipykernel_4050/4246006708.py:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  tweets = pd.read_csv('./datasets/tweets/tweets.csv', error_bad_lines = False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentSource</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>neg</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>neg</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>pos</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>neg</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I've been at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>neg</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID Sentiment SentimentSource  \\\n",
       "0       1       neg    Sentiment140   \n",
       "1       2       neg    Sentiment140   \n",
       "2       3       pos    Sentiment140   \n",
       "3       4       neg    Sentiment140   \n",
       "4       5       neg    Sentiment140   \n",
       "\n",
       "                                       SentimentText  \n",
       "0                       is so sad for my APL frie...  \n",
       "1                     I missed the New Moon trail...  \n",
       "2                            omg its already 7:30 :O  \n",
       "3  .. Omgaga. Im sooo  im gunna CRy. I've been at...  \n",
       "4           i think mi bf is cheating on me!!!   ...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv('./datasets/tweets/tweets.csv', error_bad_lines = False)\n",
    "\n",
    "tweets = tweets.head(50000)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neg</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neg</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I've been at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sentiment                                      SentimentText\n",
       "0       neg                       is so sad for my APL frie...\n",
       "1       neg                     I missed the New Moon trail...\n",
       "2       pos                            omg its already 7:30 :O\n",
       "3       neg  .. Omgaga. Im sooo  im gunna CRy. I've been at...\n",
       "4       neg           i think mi bf is cheating on me!!!   ..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = tweets.drop(columns= ['ItemID', 'SentimentSource'], axis = 1)\n",
    "\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neg', 'pos'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['Sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos    26921\n",
       "neg    23079\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 0, 'Labels')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "ax = sns.barplot(x=tweets.Sentiment.unique(), y=tweets.Sentiment.value_counts())\n",
    "\n",
    "ax.set(xlabel='Labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(tweets, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(      Sentiment                                      SentimentText\n",
       " 0           pos  @amyrenea omg so am I lol I fell asleep when i...\n",
       " 1           neg               @Adrienne_Bailon I want a shout out \n",
       " 2           neg  @Anonymousboy03 Plans for school stuff &amp; a...\n",
       " 3           neg  ... has hit a writer's block .. am loosing my ...\n",
       " 4           neg  ... trying to find people I know! I`m bored, i...\n",
       " ...         ...                                                ...\n",
       " 39995       pos   #robotpickuplines are so funny. check them out. \n",
       " 39996       pos  @annyo84 awh thankss.  yeah, i understand what...\n",
       " 39997       pos  @AmbiguityX ohh you're in twin cities?  i luv ...\n",
       " 39998       neg   Dinara lost again in Roland Garros. Why the S...\n",
       " 39999       pos  *yawn* fucking time zones shit. I'm really sic...\n",
       " \n",
       " [40000 rows x 2 columns],\n",
       "      Sentiment                                      SentimentText\n",
       " 0          pos  @aimeesays aww i hope it does fly by because J...\n",
       " 1          neg  #dontyouhate when you JUST painted yur nails a...\n",
       " 2          neg  - @EvertB which one? http://bit.ly/10o8LW, htt...\n",
       " 3          pos  *shriek* Bee almost flew here from window. I'm...\n",
       " 4          pos  @Alyssa_Milano granted if we lose it is to a w...\n",
       " ...        ...                                                ...\n",
       " 9995       neg  @aisforamylynn you're a badass for having a ba...\n",
       " 9996       pos  @acts_rox  I'm not particular about it being f...\n",
       " 9997       pos                     @@j311stp and the same to you!\n",
       " 9998       pos  .@nanere Sheila I heart you!! That &quot;Holly...\n",
       " 9999       neg   not the same without a goodnight....hm. Wish ...\n",
       " \n",
       " [10000 rows x 2 columns])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.reset_index(drop=True), test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('./datasets/tweets/train_tweets.csv', index=False)\n",
    "test.to_csv('./datasets/tweets/test_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_clean(text):\n",
    "\n",
    "    text = re.sub(r'[^A-Za-z0-9]+', ' ', text)\n",
    "    text = re.sub(r'https?:/\\/\\S+', ' ', text)\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'tagger', 'ner'])\n",
    "\n",
    "def tokenizer(s):\n",
    "    return [w.text.lower() for w in nlp(tweet_clean(s))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = torchtext.data.Field(tokenize = tokenizer)\n",
    "\n",
    "LABEL = torchtext.data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafields = [('Sentiment', LABEL), ('SentimentText', TEXT)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anishmitagar/miniforge3/envs/m1pytorch/lib/python3.9/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "trn, tst = torchtext.data.TabularDataset.splits(path= './datasets/tweets/', train= 'train_tweets.csv', test= 'test_tweets.csv', format = 'csv', skip_header = True, fields = datafields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 40000\n",
      "Number of testing examples: 10000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(trn)}')\n",
    "print(f'Number of testing examples: {len(tst)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sentiment': 'pos',\n",
       " 'SentimentText': ['amyrenea',\n",
       "  'omg',\n",
       "  'so',\n",
       "  'am',\n",
       "  'i',\n",
       "  'lol',\n",
       "  'i',\n",
       "  'fell',\n",
       "  'asleep',\n",
       "  'when',\n",
       "  'it',\n",
       "  'was',\n",
       "  'on',\n",
       "  'last',\n",
       "  'night',\n",
       "  'so',\n",
       "  'now',\n",
       "  'i',\n",
       "  'get',\n",
       "  'to',\n",
       "  'finish',\n",
       "  'it']}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(trn.examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(trn, max_size=25000, vectors=\"glove.6B.100d\", unk_init=torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 25644), ('the', 12219), ('to', 12111), ('you', 10723), ('a', 9197), ('it', 8440), ('and', 6889), ('my', 6208), ('quot', 5582), ('s', 5565), ('that', 5306), ('is', 5203), ('for', 4971), ('in', 4852), ('t', 4844), ('m', 4683), ('me', 4588), ('of', 4331), ('on', 3918), ('have', 3752), ('so', 3612), ('but', 3506), ('be', 2932), ('not', 2887), ('was', 2775), ('just', 2724), ('can', 2523), ('do', 2418), ('are', 2351), ('your', 2320), ('with', 2269), ('good', 2203), ('like', 2173), ('at', 2131), ('no', 2119), ('this', 2094), ('all', 2069), ('up', 2066), ('now', 2063), ('get', 2044), ('we', 1988), ('u', 1890), ('love', 1885), ('lol', 1864), ('too', 1826), ('what', 1760), ('out', 1742), ('know', 1664), ('nt', 1608), ('amp', 1539)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', 'i', 'the', 'to', 'you', 'a', 'it', 'and', 'my']\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.itos[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'pos': 0, 'neg': 1})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, test_iterator = torchtext.data.BucketIterator.splits((trn, tst), batch_size=64, sort_key=lambda x: len(x.SentimentText), sort_within_batch=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, text):\n",
    "\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "\n",
    "        output, hidden = self.rnn(embedded)\n",
    "\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "\n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(TEXT.vocab)\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "hidden_dim = 20\n",
    "output_dim = 1\n",
    "\n",
    "n_layers = 2\n",
    "bidirectional = True\n",
    "\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(input_dim, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(25002, 100)\n",
       "  (rnn): GRU(100, 20, num_layers=2, dropout=0.5, bidirectional=True)\n",
       "  (fc): Linear(in_features=40, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25002, 100])\n"
     ]
    }
   ],
   "source": [
    "pretrained_embedding = TEXT.vocab.vectors\n",
    "\n",
    "print(pretrained_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4184,  0.1735,  0.7379,  ..., -0.1453, -2.6076,  1.1391],\n",
       "        [-0.5602,  0.6973,  1.7147,  ...,  1.4301,  1.2774,  0.1010],\n",
       "        [-0.0465,  0.6197,  0.5665,  ..., -0.3762, -0.0325,  0.8062],\n",
       "        ...,\n",
       "        [-1.3874, -0.0674, -1.1876,  ...,  0.6008,  0.2123, -1.5581],\n",
       "        [-0.1194,  0.6886,  0.1779,  ..., -0.5569,  0.7120,  0.0788],\n",
       "        [ 0.7208,  0.7842, -0.6756,  ..., -0.6402,  0.5416, -0.9089]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight.data.copy_(pretrained_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0465,  0.6197,  0.5665,  ..., -0.3762, -0.0325,  0.8062],\n",
      "        ...,\n",
      "        [-1.3874, -0.0674, -1.1876,  ...,  0.6008,  0.2123, -1.5581],\n",
      "        [-0.1194,  0.6886,  0.1779,  ..., -0.5569,  0.7120,  0.0788],\n",
      "        [ 0.7208,  0.7842, -0.6756,  ..., -0.6402,  0.5416, -0.9089]])\n"
     ]
    }
   ],
   "source": [
    "unk_idx = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "pad_idx = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model.embedding.weight.data[unk_idx] = torch.zeros(embedding_dim)\n",
    "model.embedding.weight.data[pad_idx] = torch.zeros(embedding_dim)\n",
    "\n",
    "print(model.embedding.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch in iterator:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions = model(batch.SentimentText).squeeze(1)\n",
    "\n",
    "        loss = criterion(predictions, batch.Sentiment)\n",
    "\n",
    "        rounded_preds = torch.round(torch.sigmoid(predictions))\n",
    "        correct = (rounded_preds == batch.Sentiment).float()\n",
    "\n",
    "        acc = correct.sum() / len(correct)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.632 | Train Acc: 63.71% |\n",
      "| Epoch: 02 | Train Loss: 0.540 | Train Acc: 73.55% |\n",
      "| Epoch: 03 | Train Loss: 0.496 | Train Acc: 76.19% |\n",
      "| Epoch: 04 | Train Loss: 0.470 | Train Acc: 78.00% |\n",
      "| Epoch: 05 | Train Loss: 0.446 | Train Acc: 79.23% |\n",
      "| Epoch: 06 | Train Loss: 0.425 | Train Acc: 80.90% |\n",
      "| Epoch: 07 | Train Loss: 0.407 | Train Acc: 81.73% |\n",
      "| Epoch: 08 | Train Loss: 0.393 | Train Acc: 82.73% |\n",
      "| Epoch: 09 | Train Loss: 0.378 | Train Acc: 83.29% |\n",
      "| Epoch: 10 | Train Loss: 0.362 | Train Acc: 84.55% |\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "\n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.485 | Test Acc: 77.47%\n"
     ]
    }
   ],
   "source": [
    "epoch_loss = 0\n",
    "epoch_acc = 0\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch in test_iterator:\n",
    "\n",
    "        predictions = model(batch.SentimentText).squeeze(1)\n",
    "\n",
    "        loss = criterion(predictions, batch.Sentiment)\n",
    "\n",
    "        rounded_preds = torch.round(torch.sigmoid(predictions))\n",
    "        correct = (rounded_preds == batch.Sentiment).float()\n",
    "\n",
    "        acc = correct.sum()/len(correct)\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "test_loss = epoch_loss / len(test_iterator)\n",
    "test_acc = epoch_acc / len(test_iterator)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I love that show\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = [tok.text for tok in nlp.tokenizer(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed = [TEXT.vocab.stoi[t] for t in tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.LongTensor(indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = tensor.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = torch.sigmoid(model(tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16953828930854797"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, May 24 2022, 21:13:51) \n[Clang 13.1.6 (clang-1316.0.21.2)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "340e956ee656efd8fdfb480dc033c937d9b626f8b21073bd1b5aa2a469586ea6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
